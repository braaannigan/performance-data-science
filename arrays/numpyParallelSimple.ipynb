{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ebf29e-fe09-438c-b611-935ce387fe52",
   "metadata": {},
   "source": [
    "# Parallel loops through numpy arrays\n",
    "We look at speeding up loops through numpy arrays. In this example we have to call a third-party library in each iteration and this third-party library will only accept a subset of our total array. As we are calling a third-party library we can't apply tricks like JIT compilation.\n",
    "\n",
    "The scenario here is that we have a 3-dimensional array with dimensions (x,y,time). We will imagine that this is a time series of 2-dimensional maps of ocean salinity. Our third-party library is the seawater library. This seawater library only accepts 2-dimensional inputs so we need to loop through the time dimension and call this library on each iteration. \n",
    "\n",
    "# Libraries\n",
    "In this example we will use the built-in [Concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html), [Joblib](https://joblib.readthedocs.io/en/latest/) and [Dask](https://docs.dask.org/en/stable/) libraries.  In the case of Dask we are using the dask delayed API for parallelising the loop.\n",
    "\n",
    "# Tl;dr\n",
    "I set out recipes for running in parallel with three libraries. Comparing performance I show that for this task threading performs faster than multiprocessing. I finish up by looking at how the libraries differ in terms of how they handle exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc4ae17f-fa1a-4670-9a03-71183b9579e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor\n",
    "\n",
    "from joblib import Parallel,delayed\n",
    "import dask\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7828ad47-5ec7-4829-80ee-92c18dc92287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a956b9c-6a82-48d3-8b65-0204f7305d9f",
   "metadata": {},
   "source": [
    "# Generate data\n",
    "We generate the numpy array we're going to loop through. \n",
    "\n",
    "We use a three-dimensional array where we'll think of the dimensions as being `(x,y,time)`. The function that we're calling, however, can only accept two-dimensional inputs in `(x,y)` so we will loop through the `time` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a997fbf8-d1f3-42e6-a3d8-ad51a1da6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(xyLength:int,timesteps:int):\n",
    "    arr = np.random.standard_normal(size=(xyLength,xyLength,timesteps))\n",
    "    return arr\n",
    "\n",
    "arraySmall = generateData(xyLength=3,timesteps=3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b9265-0b8b-45e2-91ae-e121272c9b9d",
   "metadata": {},
   "source": [
    "We define the function that we are going to call in each iteration `timestepFunc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7576fb64-c8d4-4e7b-86b7-3eab94713360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestepFunc(arrTimestep:np.ndarray,timeIndex:int):\n",
    "    return np.exp(arrTimestep),timeIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225718b0-aef7-4f09-81d6-41a47a217e98",
   "metadata": {},
   "source": [
    "#### First we create a baseline non-parallelised function to do sequential processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0004a8-3687-4389-bc62-ca056d9afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialProcessing(arr:np.ndarray):\n",
    "    return np.stack(\n",
    "        [timestepFunc(arrTimestep=arr[:,:,timestep],timeIndex=timestep)[0] for timestep in range(arr.shape[2])],\n",
    "        axis=2)\n",
    "\n",
    "# Call the function\n",
    "outputSerial = serialProcessing(arr=arraySmall)\n",
    "# Check the outputs are what we expect\n",
    "np.testing.assert_array_equal(outputSerial,np.exp(arraySmall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df9d06-cf08-4d1f-aa88-f902cf003ac3",
   "metadata": {},
   "source": [
    "#### The outputs of parallel functions are not in the same order as the inputs\n",
    "\n",
    "#### Before we create parallel functions we define a helper function that will sort the list of outputs using the time index variable we added to `timestepFunc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a1d4bd-943a-4230-8148-9e747ca2dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortResults(resultList:list):\n",
    "    resultList = sorted(resultList,key=lambda x:x[1])\n",
    "    resultList = [el[0] for el in resultList]\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e179e6-292b-4e77-b62a-81d083f5415f",
   "metadata": {},
   "source": [
    "# Parallel processing\n",
    "\n",
    "## Concurrent.futures\n",
    "\n",
    "##### The built-in concurrent.futures module is a great place to start with parallel processing. It comes with both a threading and multiprocessing backend. The APIs for these backends are similar, so it's also easy to swap them out and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c11037cc-3b9f-42d3-8011-74879ee8c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e17c634f-6448-4502-8007-98d4562b32ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concurrentProcessing(arr:np.ndarray,executor,func:Callable):\n",
    "    \"\"\"\n",
    "    Iterate through the array `arr` in parallel with either the threading executor or the multiprocessing executor\n",
    "    \"\"\"\n",
    "#     Create the list that will hold the results of each iteration\n",
    "    resultList = []\n",
    "#     Set up the executor \n",
    "#   we use a with statement here to ensure the pool of threads/processes gets closed whether the jobs run successfully or not\n",
    "    with executor() as pool:\n",
    "#         Loop through the array and store the `futures` that we get from each iteration\n",
    "        futuresList = [\n",
    "             pool.submit(\n",
    "                func,\n",
    "              arr[:,:,timestep],              \n",
    "                 timestep\n",
    "                ) for timestep in range(arr.shape[2])]\n",
    "#         Gather up the completed tasks\n",
    "        done_results = concurrent.futures.as_completed(futuresList)\n",
    "#     Create the list of results\n",
    "        for _ in futuresList: \n",
    "            resultList.append(next(done_results).result())\n",
    "#         Sort the results back into their original order\n",
    "        resultList = sortResults(resultList=resultList)\n",
    "#     Convert the list of results back into a three-dimensional numpy array\n",
    "    return np.stack(resultList,axis=2)\n",
    "\n",
    "# Run the function with the multiprocessing `ProcessPoolExecutor` and check that the outputs are the same as for the serial processing\n",
    "outputConcurrent = concurrentProcessing(arr=arraySmall,executor=ProcessPoolExecutor,func=timestepFunc)\n",
    "np.testing.assert_array_equal(outputSerial,outputConcurrent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb883d-7aa4-4325-bd2b-d32ff475f270",
   "metadata": {},
   "source": [
    "## Joblib\n",
    "The joblib library can be either a reimplementation of the built-in `multiprocessing` and `threading` libraries or a wrapper for them. There are some other differences such as:\n",
    "- a different way of writing the code that you might find more readable\n",
    "- you can call ctrl-c (= hitting the stop button in a notebook) to interrupt execution of the parallel jobs\n",
    "- ability to use shared memory for large numpy arrays\n",
    "\n",
    "There has also been strong co-development of `joblib` and `scikit-learn` so `joblib` is often a good choice for parallelising machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f21284b9-5366-4b00-a0ee-d2b049ab92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joblibProcessing(arr:np.ndarray,backend = \"threading\",maxNbytes=1,nJobs:int=-1):\n",
    "#   Iterate through the third-dimension of the array in parallel\n",
    "    resultList = Parallel(backend=backend,max_nbytes=maxNbytes,n_jobs=nJobs)(delayed(timestepFunc)(arr[:,:,timestep],timestep) for timestep in range(arr.shape[2]))\n",
    "#   Sort the results back into their original order\n",
    "    resultList = sortResults(resultList=resultList)\n",
    "#     Convert the list of results back into a three-dimensional numpy array\n",
    "    return np.stack(resultList,axis=2)\n",
    "\n",
    "# Run the function with threading and check that the outputs are the same as for the serial processing\n",
    "outputJoblib = joblibProcessing(arr=arraySmall)\n",
    "np.testing.assert_array_equal(outputSerial,outputJoblib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5944f-428a-45ba-9d52-7190ddc4ae93",
   "metadata": {},
   "source": [
    "# Dask delayed\n",
    "Dask delayed wraps the `concurrent.futures` threading and multiprocessing pools we say above. As with `joblib` you can use ctrl+c to kill any subprocesses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6cdbca2-ccad-4fb2-91bf-22929c084f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daskDelayedProcessing(arr:np.ndarray):\n",
    "#   Iterate through the third-dimension of the array in parallel\n",
    "    resultList = [dask.delayed(timestepFunc,pure=False)(arr[:,:,timestep],timestep) for timestep in range(arr.shape[2])]\n",
    "#   Trigger computation of the outputs\n",
    "    resultList = dask.compute(*resultList)\n",
    "#   Sort the results back into their original order\n",
    "    resultList = sortResults(resultList=resultList)\n",
    "#     Convert the list of results back into a three-dimensional numpy array\n",
    "    return np.stack(resultList,axis=2)\n",
    "outputDask = daskDelayedProcessing(arr=arraySmall)\n",
    "np.testing.assert_array_equal(outputSerial,outputDask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda3cb7-9156-4c5f-8a10-ba020733d841",
   "metadata": {},
   "source": [
    "# Timings\n",
    "\n",
    "##### We generate a larger array here to compare performance. The numbers here should be taken with a pinch of salt - the main point is to show you how to implement these approaches so you can test the results on your own problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57ff0f93-2372-46a1-b432-d32893420178",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyLength = 200\n",
    "timesteps = 2000\n",
    "arrayLarge = generateData(xyLength=xyLength,timesteps=timesteps)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "536942f2-421c-4a95-a2be-55645c7f89a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Serial processing**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.55 s ± 974 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Multiprocessing**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Multiprocessing with Concurrent.futures**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.76 s ± 40.9 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Loky multiprocessing with Joblib**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.84 s ± 1.57 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Multiprocessing with Joblib**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2 s ± 823 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Multiprocessing with Dask**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.51 s ± 254 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Threading**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Threading with Concurrent.futures**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.51 s ± 217 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Threading with Joblib**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.55 s ± 146 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Threading with Dask**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25 s ± 1.09 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "printmd(\"**Serial processing**\")\n",
    "%timeit -n 1 -r 3 serialProcessing(arr=arrayLarge)\n",
    "printmd(\"**Multiprocessing**\")\n",
    "printmd(\"**Multiprocessing with Concurrent.futures**\")\n",
    "%timeit -n 1 -r 3 concurrentProcessing(arr=arrayLarge,executor=ProcessPoolExecutor)\n",
    "printmd(\"**Loky multiprocessing with Joblib**\")\n",
    "%timeit -n 1 -r 3 joblibProcessing(arr=arrayLarge,backend=\"loky\")\n",
    "printmd(\"**Multiprocessing with Joblib**\")\n",
    "%timeit -n 1 -r 3 joblibProcessing(arr=arrayLarge,backend=\"multiprocessing\")\n",
    "printmd(\"**Multiprocessing with Dask**\")\n",
    "dask.config.set(scheduler='processes')  # overwrite default with processes scheduler\n",
    "%timeit -n 1 -r 3 daskDelayedProcessing(arr=arrayLarge)\n",
    "printmd(\"**Threading**\")\n",
    "printmd(\"**Threading with Concurrent.futures**\")\n",
    "%timeit -n 1 -r 3 concurrentProcessing(arr=arrayLarge,executor=ThreadPoolExecutor)\n",
    "printmd(\"**Threading with Joblib**\")\n",
    "%timeit -n 1 -r 3 joblibProcessing(arr=arrayLarge,backend=\"threading\")\n",
    "\n",
    "printmd(\"**Threading with Dask**\")\n",
    "dask.config.set(scheduler='threads')  # overwrite default with threads scheduler\n",
    "%timeit -n 1 -r 3 daskDelayedProcessing(arr=arrayLarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f571472-5b5d-4bbf-b43d-11909472e6d1",
   "metadata": {},
   "source": [
    "## Standard numpy\n",
    "Our `timestepFunc` is a stand-in for a third-party library where we can't work on the entire array in one go. But in our dummy example we can also compare the performance of the underlying numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3eb9dc10-0f6d-4008-a5ca-94537e97ff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Standard numpy**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05 s ± 21.1 ms per loop (mean ± std. dev. of 25 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "printmd(\"**Standard numpy**\")\n",
    "%timeit -n 1 -r 25 np.exp(arrayLarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac2beb-e03e-4e32-bd0a-6ef72ba087c9",
   "metadata": {},
   "source": [
    "The time taken on my computer is just half the time taken by the fastest parallel loop. This shows that you need to set standard numpy as your baseline for comparison in cases where it is possible to process the array directly with numpy. \n",
    "\n",
    "If we wanted an easier way to parallelise this operation than writing explicit loops, we could use a library with built-in parallelisation like numExpr or Dask. I will explore these approaches in more detail in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0f77eae8-a6a1-4d63-8eae-7229f3b2a9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**NumExpr**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305 ms ± 30.9 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "printmd(\"**NumExpr**\")\n",
    "import numexpr as ne\n",
    "%timeit -n 1 -r 5 ne.evaluate(\"exp(arrayLarge)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0bcf3eae-b84e-48bc-b931-4d14d474e972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Dask Array**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "printmd(\"**Dask Array**\")\n",
    "daskArray = da.from_array(arrayLarge)\n",
    "%timeit -n 1 -r 5 da.exp(daskArray).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f0bec-93e0-4d9d-937d-512c6660974e",
   "metadata": {},
   "source": [
    "# Differences between the parallel libraries: exception handling\n",
    "\n",
    "If each of the parallel libraries wraps the built-in `multiprocessing` and `threading` modules you might ask what the difference between them is. We saw above that one difference is that `joblib` and `dask` are more responsive to stop signals from ctrl+c (the stop button in a notebook).\n",
    "\n",
    "Another important difference is how they **respond to an exception being raised** in a child process. When building automated applications to run for my clients I generally want everything to stop if an error occurs rather than some processes failing and some continuing. We can test how these libraries behave by defining an alternative function that will raise a `ValueError` (giving a big pink box in jupyter) instead of running to completion. In the following cell we'll define that function and call it to make sure it raise the exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd2f014e-46f7-4291-a4ae-7795ea267a50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Throw a ValueError to see how the libraries respond",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimestepRaiseExceptionFunc\u001b[39m(arrTimestep:np\u001b[38;5;241m.\u001b[39mndarray,timeIndex:\u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThrow a ValueError to see how the libraries respond\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m----> 4\u001b[0m \u001b[43mtimestepRaiseExceptionFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrTimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marraySmall\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtimeIndex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36mtimestepRaiseExceptionFunc\u001b[0;34m(arrTimestep, timeIndex)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimestepRaiseExceptionFunc\u001b[39m(arrTimestep:np\u001b[38;5;241m.\u001b[39mndarray,timeIndex:\u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThrow a ValueError to see how the libraries respond\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Throw a ValueError to see how the libraries respond"
     ]
    }
   ],
   "source": [
    "def timestepRaiseExceptionFunc(arrTimestep:np.ndarray,timeIndex:int):\n",
    "    raise ValueError(\"Throw a ValueError to see how the libraries respond\") \n",
    "\n",
    "timestepRaiseExceptionFunc(arrTimestep=arraySmall[:,:,0],timeIndex=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbe2a6-1162-475a-97ba-e5c2ab99a25c",
   "metadata": {},
   "source": [
    "### Concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0706005b-8688-4adb-a3e4-0de16bf89bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concurrentProcessingRaiseException(arr:np.ndarray,executor):\n",
    "    \"\"\"\n",
    "    Iterate through the array `arr` in parallel with either the threading executor or the multiprocessing executor\n",
    "     \"\"\"\n",
    "#   Create the list that will hold the results of each iteration\n",
    "    executor = ThreadPoolExecutor\n",
    "    resultList = []\n",
    "    arr = arraySmall\n",
    "    #     Set up the executor \n",
    "    #   we use a with statement here to ensure the pool of threads/processes gets closed whether the jobs run successfully or not\n",
    "    with executor() as pool:\n",
    "    #         Loop through the array and store the `futures` that we get from each iteration\n",
    "        futuresList = [\n",
    "             pool.submit(\n",
    "                timestepRaiseExceptionFunc,\n",
    "              arr[:,:,timestep],              \n",
    "                 timestep\n",
    "                ) for timestep in range(arr.shape[2])]\n",
    "    #         Gather up the completed tasks\n",
    "        done_results = concurrent.futures.as_completed(futuresList)\n",
    "\n",
    "concurrentProcessingRaiseException(arr=arraySmall,executor=ThreadPoolExecutor)\n",
    "concurrentProcessingRaiseException(arr=arraySmall,executor=ProcessPoolExecutor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e609e-78a4-4cd7-a28d-be2f2e607ba9",
   "metadata": {},
   "source": [
    "Uh oh: we didn't get an exception here. Let's see what happens with joblib and dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c636f2-9926-45ab-a378-98acc32b3096",
   "metadata": {},
   "source": [
    "### Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bac40ec0-0201-4633-966f-6f9b95405dfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Throw a ValueError to see how the libraries respond",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     Convert the list of results back into a three-dimensional numpy array\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(resultList,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mjoblibProcessingRaiseException\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marraySmall\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mjoblibProcessingRaiseException\u001b[0;34m(arr, backend, maxNbytes, nJobs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoblibProcessingRaiseException\u001b[39m(arr:np\u001b[38;5;241m.\u001b[39mndarray,backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreading\u001b[39m\u001b[38;5;124m\"\u001b[39m,maxNbytes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,nJobs:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#   Iterate through the third-dimension of the array in parallel\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     resultList \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_nbytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxNbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnJobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestepRaiseExceptionFunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#   Sort the results back into their original order\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     resultList \u001b[38;5;241m=\u001b[39m sortResults(resultList\u001b[38;5;241m=\u001b[39mresultList)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/_parallel_backends.py:595\u001b[0m, in \u001b[0;36mSafeFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;66;03m# We capture the KeyboardInterrupt and reraise it as\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;66;03m# something different, as multiprocessing does not\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;66;03m# interrupt processing for a KeyboardInterrupt\u001b[39;00m\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WorkerInterrupt() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36mtimestepRaiseExceptionFunc\u001b[0;34m(arrTimestep, timeIndex)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimestepRaiseExceptionFunc\u001b[39m(arrTimestep:np\u001b[38;5;241m.\u001b[39mndarray,timeIndex:\u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeIndex \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThrow a ValueError to see how the libraries respond\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(arrTimestep),timeIndex\n",
      "\u001b[0;31mValueError\u001b[0m: Throw a ValueError to see how the libraries respond"
     ]
    }
   ],
   "source": [
    "def joblibProcessingRaiseException(arr:np.ndarray,backend = \"threading\",maxNbytes=1,nJobs:int=-1):\n",
    "#   Iterate through the third-dimension of the array in parallel\n",
    "    resultList = Parallel(backend=backend,max_nbytes=maxNbytes,n_jobs=nJobs)(delayed(timestepRaiseExceptionFunc)(arr[:,:,timestep],timestep) for timestep in range(arr.shape[2]))\n",
    "#   Sort the results back into their original order\n",
    "    resultList = sortResults(resultList=resultList)\n",
    "#     Convert the list of results back into a three-dimensional numpy array\n",
    "    return np.stack(resultList,axis=2)\n",
    "\n",
    "\n",
    "joblibProcessingRaiseException(arr=arraySmall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9e037-3e94-4952-a16e-e376339b36c3",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03ab4674-8be2-49f3-bbed-5a6f64fbae68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Throw a ValueError to see how the libraries respond",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     Convert the list of results back into a three-dimensional numpy array\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(resultList,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdaskDelayedProcessingRaiseException\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marraySmall\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mdaskDelayedProcessingRaiseException\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m      3\u001b[0m     resultList \u001b[38;5;241m=\u001b[39m [dask\u001b[38;5;241m.\u001b[39mdelayed(timestepRaiseExceptionFunc,pure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)(arr[:,:,timestep],timestep) \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#   Trigger computation of the outputs\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     resultList \u001b[38;5;241m=\u001b[39m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresultList\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#   Sort the results back into their original order\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     resultList \u001b[38;5;241m=\u001b[39m sortResults(resultList\u001b[38;5;241m=\u001b[39mresultList)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/dask/base.py:571\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    569\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 571\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/dask/threaded.py:79\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     77\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 79\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/dask/local.py:507\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    509\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/dask/local.py:315\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/dask/local.py:220\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    222\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mtimestepRaiseExceptionFunc\u001b[0;34m(arrTimestep, timeIndex)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtimestepRaiseExceptionFunc\u001b[39m(arrTimestep:np\u001b[38;5;241m.\u001b[39mndarray,timeIndex:\u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThrow a ValueError to see how the libraries respond\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(arrTimestep),timeIndex\n",
      "\u001b[0;31mValueError\u001b[0m: Throw a ValueError to see how the libraries respond"
     ]
    }
   ],
   "source": [
    "def daskDelayedProcessingRaiseException(arr:np.ndarray):\n",
    "#   Iterate through the third-dimension of the array in parallel\n",
    "    resultList = [dask.delayed(timestepRaiseExceptionFunc,pure=False)(arr[:,:,timestep],timestep) for timestep in range(arr.shape[2])]\n",
    "#   Trigger computation of the outputs\n",
    "    resultList = dask.compute(*resultList)\n",
    "#   Sort the results back into their original order\n",
    "    resultList = sortResults(resultList=resultList)\n",
    "#     Convert the list of results back into a three-dimensional numpy array\n",
    "    return np.stack(resultList,axis=2)\n",
    "\n",
    "daskDelayedProcessingRaiseException(arr=arraySmall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76186b7a-2137-461e-a389-d5f9ff39b98c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We see that `concurrent.futures` didn't raise an exception despite the `ValueError` in each iteration. Instead with both the threading and multiprocessing the function just returns `None`.\n",
    "\n",
    "On the other hand `joblib` and `dask` did what we wanted and stopped execution of the parent function when the `ValueError` occurred. This example gives you a sense of the subtlties that emerge when you run parallel operations in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57767c3-637a-407e-af2b-449dd4a7c92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
